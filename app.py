from fastapi import FastAPI
from db.redis_client import r, create_index, index_exists
from utils.pdf_loader import load_pdf_text, chunk_text
from utils.embed import get_embeddings, rerank
from models.query_model import QueryRequest
from config import PDF_PATH, CHUNK_SIZE, CHUNK_OVERLAP, REDIS_INDEX, TOP_K
import numpy as np
import time
from redis.commands.search.query import Query
from contextlib import asynccontextmanager
from utils.llm_client import ask_llm

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("üöÄ Initializing PDF + Redis setup...")

    # Startup code
    if not index_exists():
        full_text = load_pdf_text(PDF_PATH)
        print("‚úÖ Loaded PDF")
        chunks = chunk_text(full_text, CHUNK_SIZE, CHUNK_OVERLAP)
        print("‚úÖ Loaded Chunks")
        embeddings = get_embeddings(chunks)
        print("‚úÖ Loaded Embeddings")
        vector_dim = len(embeddings[0])

        create_index(vector_dim)
        print("‚úÖ Created Index")
        start = time.time()
        for i, emb in enumerate(embeddings):
            r.hset(f"chunk:{i}", mapping={
                "content": chunks[i],
                "embedding": np.array(emb, dtype=np.float16).tobytes()
            })
        print(f"‚úÖ Inserted {len(chunks)} chunks into Redis in {time.time()-start:.2f}s")
    else:
        print("‚ÑπÔ∏è Redis index already exists ‚Äî skipping embedding.")

    yield  # <- FastAPI now runs

    # Shutdown code
    print("Server is stopping...")

# Initialize app with lifespan
app = FastAPI(lifespan=lifespan)


@app.post("/query")
def query_pdf(request: QueryRequest):
    query = request.query

    q_emb = get_embeddings([query])[0]
    q_vector = np.array(q_emb, dtype=np.float16).tobytes()

    q = Query(f"*=>[KNN {TOP_K} @embedding $vector AS score]") \
        .return_fields("content", "score") \
        .sort_by("score", asc=True) \
        .paging(0, TOP_K)

    results = r.ft(REDIS_INDEX).search(q, query_params={"vector": q_vector})

    # Extract docs for reranking
    documents = [doc.content for doc in results.docs]

    context = "\n\n".join(documents)

    prompt = f"""
    Answer the question naturally in your own words **only** from the context below.
    If you don't find the answer, then say "The document does not contain this information."

    Context:
    {context}

    Question: {query}
    Answer:
    """

    answer = ask_llm(prompt)

    return {"answer": answer}
